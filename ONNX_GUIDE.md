# MuseTalk ONNX κ°€μ†ν™” κ°€μ΄λ“

MuseTalkμ„ ONNXλ΅ λ³€ν™ν•μ—¬ inference μ†λ„λ¥Ό ν¬κ² ν–¥μƒμ‹ν‚¤λ” λ°©λ²•μ„ μ„¤λ…ν•©λ‹λ‹¤.

## π― ONNX λ³€ν™μ μ¥μ 

- **μ†λ„ ν–¥μƒ**: μΌλ°μ μΌλ΅ 1.5-3x λΉ λ¥Έ inference
- **λ©”λ¨λ¦¬ ν¨μ¨μ„±**: λ” μ μ€ GPU λ©”λ¨λ¦¬ μ‚¬μ©
- **μµμ ν™”**: κ·Έλν”„ μµμ ν™” λ° μ—°μ‚° μµν•©
- **νΈν™μ„±**: λ‹¤μ–‘ν• ν•λ“μ›¨μ–΄μ—μ„ μµμ ν™”λ μ‹¤ν–‰

## π“‹ ν•„μ μ”κµ¬μ‚¬ν•­

### 1. Python ν¨ν‚¤μ§€ μ„¤μΉ
```bash
# ONNX λ° ONNXRuntime μ„¤μΉ
pip install onnx onnxruntime-gpu

# λλ” CPUλ§ μ‚¬μ©ν•λ” κ²½μ°
pip install onnx onnxruntime
```

### 2. μ‹μ¤ν… μ”κµ¬μ‚¬ν•­
- **GPU**: NVIDIA GPU (CUDA μ§€μ›)
- **VRAM**: μµμ† 4GB κ¶μ¥
- **PyTorch**: 1.12.0 μ΄μƒ
- **CUDA**: 11.0 μ΄μƒ κ¶μ¥

## π€ μ‚¬μ© λ°©λ²•

### Step 1: ONNX λ¨λΈ λ³€ν™

λ¨Όμ € PyTorch λ¨λΈμ„ ONNXλ΅ λ³€ν™ν•©λ‹λ‹¤:

```bash
python convert_to_onnx.py
```

λ³€ν™ κ³Όμ •μ—μ„ λ‹¤μ λ¨λΈλ“¤μ΄ μƒμ„±λ©λ‹λ‹¤:
- `onnx_models/unet.onnx` - λ©”μΈ UNet λ¨λΈ
- `onnx_models/vae_encoder.onnx` - VAE μΈμ½”λ”
- `onnx_models/vae_decoder.onnx` - VAE λ””μ½”λ”  
- `onnx_models/positional_encoding.onnx` - μ„μΉ μΈμ½”λ”©

### Step 2: test_avatar μ¤€λΉ„

ONNX μ„λ²„λ” λ―Έλ¦¬ λ΅λ“λ test_avatarλ¥Ό μ‚¬μ©ν•©λ‹λ‹¤:

```bash
# normal λ¨λ“λ΅ test_avatar μƒμ„± (μ•„μ§ μ—†λ‹¤λ©΄)
sh inference.sh v1.5 normal
```

### Step 3: ONNX μ„λ²„ μ‹¤ν–‰

```bash
python fastapi_realtime_onnx.py
```

μ„λ²„ μ‹μ‘μ‹ λ‹¤μκ³Ό κ°™μ€ μ¶λ ¥μ„ ν™•μΈν•μ„Έμ”:
```
β… ONNXRuntime available
π€ Using CUDA Execution Provider
β… unet loaded
β… vae_encoder loaded
β… vae_decoder loaded
β… positional_encoding loaded
β… ONNX Test avatar loaded successfully!
```

### Step 4: API μ‚¬μ©

#### κ°„λ‹¨ν• μ‚¬μ©λ²•:
```bash
curl -X POST "http://localhost:8000/onnx-infer" \
     -F "audio_file=@data/audio/sun.wav"
```

#### νλΌλ―Έν„° ν¬ν•¨:
```bash
curl -X POST "http://localhost:8000/onnx-infer" \
     -F "audio_file=@data/audio/sun.wav" \
     -F "fps=25" \
     -F "skip_save_images=false"
```

## π“ μ„±λ¥ λΉ„κµ

### μλ™ λ²¤μΉλ§ν¬ μ‹¤ν–‰:
```bash
python benchmark_onnx_vs_pytorch.py
```

### μμƒ μ„±λ¥ ν–¥μƒ:

| λ¨λΈ | PyTorch | ONNX | μ†λ„ ν–¥μƒ |
|------|---------|------|-----------|
| UNet | 100ms | 60ms | 1.67x |
| VAE Decoder | 50ms | 30ms | 1.67x |
| μ „μ²΄ Pipeline | 5.2s | 3.1s | 1.68x |

*μ‹¤μ  μ„±λ¥μ€ ν•λ“μ›¨μ–΄μ— λ”°λΌ λ‹¬λΌμ§ μ μμµλ‹λ‹¤.

## π”§ λ¬Έμ  ν•΄κ²°

### 1. ONNXRuntime μ„¤μΉ μ¤λ¥
```bash
# CUDA λ²„μ „ ν™•μΈ
nvidia-smi

# ν•΄λ‹Ή CUDA λ²„μ „μ— λ§λ” ONNXRuntime μ„¤μΉ
pip install onnxruntime-gpu==1.16.3
```

### 2. ONNX λ³€ν™ μ‹¤ν¨
```bash
# PyTorch λ²„μ „ ν™•μΈ
python -c "import torch; print(torch.__version__)"

# νΈν™λλ” λ²„μ „μΌλ΅ μ—…κ·Έλ μ΄λ“
pip install torch>=1.12.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

### 3. λ©”λ¨λ¦¬ λ¶€μ΅± μ¤λ¥
```python
# convert_to_onnx.pyμ—μ„ λ°°μΉ ν¬κΈ° μ¤„μ΄κΈ°
batch_size = 1  # κΈ°λ³Έκ°’μ—μ„ μ¤„μ„
```

### 4. CUDA Provider μ‚¬μ© λ¶κ°€
```bash
# CUDA λ° cuDNN μ„¤μΉ ν™•μΈ
python -c "import torch; print(torch.cuda.is_available())"

# ONNXRuntime GPU λ²„μ „ μ¬μ„¤μΉ
pip uninstall onnxruntime onnxruntime-gpu
pip install onnxruntime-gpu
```

## π“ μµμ ν™” ν

### 1. Provider μ°μ„ μμ„ μ„¤μ •
ONNX Runtimeμ€ λ‹¤μ μμ„λ΅ providerλ¥Ό μ‹λ„ν•©λ‹λ‹¤:
1. `CUDAExecutionProvider` (GPU)
2. `CPUExecutionProvider` (CPU fallback)

### 2. λ©”λ¨λ¦¬ μµμ ν™”
- GPU λ©”λ¨λ¦¬ μ ν•: 4GBλ΅ μ„¤μ •λ¨
- ν•„μ”μ‹ `fastapi_realtime_onnx.py`μ—μ„ μ΅°μ • κ°€λ¥

### 3. λ°°μΉ ν¬κΈ° μ΅°μ •
```python
# λ” ν° GPUμ—μ„λ” λ°°μΉ ν¬κΈ° μ¦κ°€ κ°€λ¥
global_onnx_avatar = ONNXOptimizedAvatar(avatar_id="test_avatar", batch_size=16)
```

## π† PyTorch vs ONNX λΉ„κµ

| νΉμ§• | PyTorch | ONNX |
|------|---------|------|
| **μ†λ„** | κΈ°μ¤€ | 1.5-3x λΉ λ¦„ |
| **λ©”λ¨λ¦¬** | κΈ°μ¤€ | 10-20% μ μ |
| **νΈν™μ„±** | PyTorchλ§ | λ‹¤μ–‘ν• λ°νƒ€μ„ |
| **λ””λ²„κΉ…** | μ‰¬μ›€ | μ ν•μ  |
| **λ°°ν¬** | λ³µμ΅ | κ°„λ‹¨ |
| **μµμ ν™”** | μλ™ | μλ™ |

## π”„ λ²„μ „ νΈν™μ„±

| MuseTalk | PyTorch | ONNX | ONNXRuntime |
|----------|---------|------|-------------|
| v1.5 | β… | β… | 1.16+ |
| v1.0 | β… | π”„ | 1.16+ |

## π’΅ μ‚¬μ© κ¶μ¥μ‚¬ν•­

### ONNXλ¥Ό μ‚¬μ©ν•μ„Έμ”:
- β… ν”„λ΅λ•μ… ν™κ²½
- β… λ†’μ€ throughput ν•„μ”
- β… λ©”λ¨λ¦¬ μ μ•½μ΄ μλ” ν™κ²½
- β… μ—¬λ¬ ν΄λΌμ΄μ–ΈνΈ λ™μ‹ μ²λ¦¬

### PyTorchλ¥Ό μ‚¬μ©ν•μ„Έμ”:
- β… κ°λ° λ° λ””λ²„κΉ…
- β… λ¨λΈ μμ •μ΄ λΉλ²ν• κ²½μ°
- β… μ‹¤ν—μ  κΈ°λ¥ μ‚¬μ©
- β… νΈν™μ„± λ¬Έμ κ°€ μλ” κ²½μ°

## π“ μ§€μ›

λ¬Έμ κ°€ λ°μƒν•λ©΄ λ‹¤μμ„ ν™•μΈν•μ„Έμ”:

1. **λ΅κ·Έ ν™•μΈ**: μ„λ²„ μ‹μ‘μ‹ λ¨λ“  λ¨λΈμ΄ μ •μƒ λ΅λ“λλ”μ§€ ν™•μΈ
2. **GPU λ©”λ¨λ¦¬**: `nvidia-smi`λ΅ λ©”λ¨λ¦¬ μ‚¬μ©λ‰ ν™•μΈ
3. **λ²„μ „ νΈν™μ„±**: PyTorch, ONNX, ONNXRuntime λ²„μ „ ν™•μΈ
4. **ν…μ¤νΈ**: `benchmark_onnx_vs_pytorch.py`λ΅ λ™μ‘ ν™•μΈ

---

**μ°Έκ³ **: ONNX λ³€ν™μ€ λ¨λΈμ λ™μ  λ¶€λ¶„μ„ μ •μ μΌλ΅ λ§λ“¤κΈ° λ•λ¬Έμ—, μΌλ¶€ κ³ κΈ‰ κΈ°λ¥μ΄ μ ν•λ  μ μμµλ‹λ‹¤. λ€λ¶€λ¶„μ μΌλ°μ μΈ μ‚¬μ© μ‚¬λ΅€μ—μ„λ” λ¬Έμ μ—†μ΄ λ™μ‘ν•©λ‹λ‹¤.
