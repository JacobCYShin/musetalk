#!/usr/bin/env python3
"""
MuseTalk Realtime Optimized FastAPI Server (Audio Processing Optimized)

ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì†ë„ë¥¼ ìµœì í™”í•œ ë²„ì „ì…ë‹ˆë‹¤.

ìµœì í™” ì‚¬í•­:
1. Whisper ëª¨ë¸ì„ í•œ ë²ˆë§Œ ë¡œë“œí•˜ê³  ì¬ì‚¬ìš©
2. ì˜¤ë””ì˜¤ ì²˜ë¦¬ ë°°ì¹˜ í¬ê¸° ìµœì í™”
3. GPU ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ê°œì„ 
4. ë¶ˆí•„ìš”í•œ ì¤‘ê°„ ì²˜ë¦¬ ê³¼ì • ì œê±°
"""

import uvicorn
from fastapi import FastAPI, UploadFile, Form, HTTPException
from fastapi.responses import FileResponse
import tempfile, os, torch
import json
import pickle
import glob
import time
from transformers import WhisperModel
from musetalk.utils.utils import load_all_model
from musetalk.utils.audio_processor import AudioProcessor
from musetalk.utils.face_parsing import FaceParsing
from musetalk.utils.preprocessing import read_imgs
import scripts.realtime_inference as realtime_inference

# ---- 1) í™˜ê²½ ì„¤ì • ----
class Args:
    pass

args = Args()
args.version = "v15"
args.extra_margin = 10
args.parsing_mode = "jaw"
args.audio_padding_length_left = 2
args.audio_padding_length_right = 2
args.skip_save_images = False

# realtime_inference ëª¨ë“ˆì— args ì „ì—­ ë“±ë¡
realtime_inference.args = args

# ---- 2) ëª¨ë¸ ë¡œë“œ ----
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(f"ğŸ”§ Using device: {device}")

vae, unet, pe = load_all_model(
    unet_model_path="./models/musetalkV15/unet.pth",
    vae_type="sd-vae",
    unet_config="./models/musetalkV15/musetalk.json",
    device=device,
)

pe = pe.half().to(device)
vae.vae = vae.vae.half().to(device)
unet.model = unet.model.half().to(device)

timesteps = torch.tensor([0], device=device)

# ---- 3) ìµœì í™”ëœ ì˜¤ë””ì˜¤ í”„ë¡œì„¸ì„œ ----
class OptimizedAudioProcessor(AudioProcessor):
    """ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì†ë„ë¥¼ ìµœì í™”í•œ AudioProcessor"""
    
    def __init__(self, feature_extractor_path="./models/whisper"):
        super().__init__(feature_extractor_path)
        # Whisper encoder ì‚¬ì „ ë¡œë“œ ë° ìµœì í™”
        self._warmup_models()
    
    def _warmup_models(self):
        """ëª¨ë¸ ì›Œë°ì—…ìœ¼ë¡œ ì²« ë²ˆì§¸ ì²˜ë¦¬ ì§€ì—° ê°ì†Œ"""
        print("ğŸ”¥ Warming up audio models...")
        dummy_audio = torch.zeros((1, 80, 3000), dtype=torch.float16).to(device)
        try:
            # Warmup feature extractor
            import numpy as np
            dummy_wav = np.zeros(16000, dtype=np.float32)  # 1ì´ˆ ë”ë¯¸ ì˜¤ë””ì˜¤
            _ = self.feature_extractor(
                dummy_wav,
                return_tensors="pt", 
                sampling_rate=16000
            )
            print("âœ… Audio models warmed up!")
        except Exception as e:
            print(f"âš ï¸  Warmup failed (normal): {e}")
    
    def get_audio_feature_fast(self, wav_path, weight_dtype=None):
        """ë¹ ë¥¸ ì˜¤ë””ì˜¤ íŠ¹ì„± ì¶”ì¶œ"""
        if not os.path.exists(wav_path):
            return None
        
        start_time = time.time()
        
        # librosa ë¡œë”© ìµœì í™”
        import librosa
        librosa_output, sampling_rate = librosa.load(wav_path, sr=16000, mono=True)
        load_time = time.time() - start_time
        
        # Feature extraction ìµœì í™”
        feature_start = time.time()
        
        # 30ì´ˆ ì²­í¬ ëŒ€ì‹  ë” ì‘ì€ ì²­í¬ë¡œ ì²˜ë¦¬í•˜ì—¬ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í–¥ìƒ
        chunk_duration = 10  # 10ì´ˆì”© ì²˜ë¦¬
        segment_length = chunk_duration * sampling_rate
        segments = [librosa_output[i:i + segment_length] 
                   for i in range(0, len(librosa_output), segment_length)]

        features = []
        for i, segment in enumerate(segments):
            # ë¹ˆ ì„¸ê·¸ë¨¼íŠ¸ ì²˜ë¦¬
            if len(segment) == 0:
                continue
                
            # ì§§ì€ ì„¸ê·¸ë¨¼íŠ¸ íŒ¨ë”©
            if len(segment) < segment_length:
                segment = np.pad(segment, (0, segment_length - len(segment)), 'constant')
            
            audio_feature = self.feature_extractor(
                segment,
                return_tensors="pt",
                sampling_rate=sampling_rate
            ).input_features
            
            if weight_dtype is not None:
                audio_feature = audio_feature.to(dtype=weight_dtype)
            features.append(audio_feature)

        feature_time = time.time() - feature_start
        total_time = time.time() - start_time
        
        print(f"ğŸ“Š Audio processing breakdown:")
        print(f"   - Loading: {load_time*1000:.1f}ms")
        print(f"   - Feature extraction: {feature_time*1000:.1f}ms") 
        print(f"   - Total: {total_time*1000:.1f}ms")

        return features, len(librosa_output)

# ìµœì í™”ëœ ì˜¤ë””ì˜¤ í”„ë¡œì„¸ì„œ ì‚¬ìš©
audio_processor = OptimizedAudioProcessor(feature_extractor_path="./models/whisper")
weight_dtype = unet.model.dtype

# Whisper ëª¨ë¸ ë¡œë“œ ë° ìµœì í™”
print("ğŸ¤– Loading Whisper model...")
whisper_start = time.time()
whisper = WhisperModel.from_pretrained("./models/whisper")
whisper = whisper.to(device=device, dtype=weight_dtype).eval()
whisper.requires_grad_(False)

# Whisper ëª¨ë¸ ìµœì í™”
if hasattr(whisper, 'encoder'):
    # ì»´íŒŒì¼ ìµœì í™” (PyTorch 2.0+)
    try:
        whisper.encoder = torch.compile(whisper.encoder, mode="reduce-overhead")
        print("âœ… Whisper encoder compiled for speed")
    except:
        print("âš ï¸  Whisper compilation not available")

whisper_load_time = time.time() - whisper_start
print(f"âœ… Whisper loaded in {whisper_load_time:.2f}s")

fp = FaceParsing(left_cheek_width=90, right_cheek_width=90)

# ---- 4) ì „ì—­ ê°ì²´ë¥¼ realtime_inference ëª¨ë“ˆì— ì£¼ì… ----
realtime_inference.vae = vae
realtime_inference.unet = unet
realtime_inference.pe = pe
realtime_inference.fp = fp
realtime_inference.audio_processor = audio_processor
realtime_inference.weight_dtype = weight_dtype
realtime_inference.whisper = whisper
realtime_inference.device = device
realtime_inference.timesteps = timesteps

# ---- 5) ìµœì í™”ëœ Avatar í´ë˜ìŠ¤ ----
@torch.no_grad()
class SuperOptimizedAvatar:
    """ê·¹ë„ë¡œ ìµœì í™”ëœ Avatar í´ë˜ìŠ¤"""
    
    def __init__(self, avatar_id, batch_size=20):
        self.avatar_id = avatar_id
        self.batch_size = batch_size
        
        # ê²½ë¡œ ì„¤ì •
        if args.version == "v15":
            self.base_path = f"./results/{args.version}/avatars/{avatar_id}"
        else:
            self.base_path = f"./results/avatars/{avatar_id}"
            
        self.avatar_path = self.base_path
        self.full_imgs_path = f"{self.avatar_path}/full_imgs"
        self.coords_path = f"{self.avatar_path}/coords.pkl"
        self.latents_out_path = f"{self.avatar_path}/latents.pt"
        self.video_out_path = f"{self.avatar_path}/vid_output/"
        self.mask_out_path = f"{self.avatar_path}/mask"
        self.mask_coords_path = f"{self.avatar_path}/mask_coords.pkl"
        self.avatar_info_path = f"{self.avatar_path}/avator_info.json"
        
        self.idx = 0
        self.load_precomputed_data()

    def load_precomputed_data(self):
        """ê¸°ì¡´ ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ë¹ ë¥´ê²Œ ë¡œë“œ"""
        if not os.path.exists(self.avatar_path):
            raise FileNotFoundError(f"Avatar '{self.avatar_id}' does not exist.")
        
        load_start = time.time()
        print(f"âš¡ Loading precomputed data for avatar: {self.avatar_id}")
        
        # í•„ìˆ˜ íŒŒì¼ë“¤ ì²´í¬
        required_files = [self.coords_path, self.latents_out_path, self.mask_coords_path]
        for file_path in required_files:
            if not os.path.exists(file_path):
                raise FileNotFoundError(f"Required file not found: {file_path}")
        
        # ë³‘ë ¬ ë¡œë”©ìœ¼ë¡œ ì†ë„ ìµœì í™”
        print("ğŸ“¦ Loading latents and coordinates...")
        
        # Latents ë¡œë“œ (ê°€ì¥ í° íŒŒì¼)
        latent_start = time.time()
        self.input_latent_list_cycle = torch.load(self.latents_out_path, map_location=device)
        latent_time = time.time() - latent_start
        
        # Coordinates ë¡œë“œ
        coord_start = time.time()
        with open(self.coords_path, 'rb') as f:
            self.coord_list_cycle = pickle.load(f)
        with open(self.mask_coords_path, 'rb') as f:
            self.mask_coords_list_cycle = pickle.load(f)
        coord_time = time.time() - coord_start
        
        # ì´ë¯¸ì§€ ë¡œë“œ (í•„ìš”í•œ ê²½ìš°ì—ë§Œ)
        img_start = time.time()
        input_img_list = sorted(glob.glob(os.path.join(self.full_imgs_path, '*.[jpJP][pnPN]*[gG]')),
                               key=lambda x: int(os.path.splitext(os.path.basename(x))[0]))
        self.frame_list_cycle = read_imgs(input_img_list)
        
        input_mask_list = sorted(glob.glob(os.path.join(self.mask_out_path, '*.[jpJP][pnPN]*[gG]')),
                                key=lambda x: int(os.path.splitext(os.path.basename(x))[0]))
        self.mask_list_cycle = read_imgs(input_mask_list)
        img_time = time.time() - img_start
        
        total_load_time = time.time() - load_start
        
        print(f"ğŸ“Š Loading time breakdown:")
        print(f"   - Latents: {latent_time:.2f}s")
        print(f"   - Coordinates: {coord_time:.2f}s")
        print(f"   - Images: {img_time:.2f}s")
        print(f"   - Total: {total_load_time:.2f}s")
        print(f"âœ… Loaded {len(self.frame_list_cycle)} frames, {len(self.input_latent_list_cycle)} latents")

    def inference(self, audio_path, out_vid_name, fps=25, skip_save_images=False):
        """ìµœì í™”ëœ inference"""
        import threading
        import queue
        import copy
        import numpy as np
        import cv2
        from tqdm import tqdm
        from musetalk.utils.utils import datagen
        from musetalk.utils.blending import get_image_blending
        
        os.makedirs(self.avatar_path + '/tmp', exist_ok=True)
        total_start = time.time()
        print("ğŸš€ Starting super-optimized realtime inference...")
        
        # ìµœì í™”ëœ ì˜¤ë””ì˜¤ ì²˜ë¦¬
        audio_start = time.time()
        whisper_input_features, librosa_length = audio_processor.get_audio_feature_fast(
            audio_path, weight_dtype=weight_dtype)
        
        whisper_chunks = audio_processor.get_whisper_chunk(
            whisper_input_features, device, weight_dtype, whisper,
            librosa_length, fps=fps,
            audio_padding_length_left=args.audio_padding_length_left,
            audio_padding_length_right=args.audio_padding_length_right,
        )
        audio_time = time.time() - audio_start
        print(f"âš¡ Optimized audio processing: {audio_time*1000:.1f}ms")
        
        # Inference
        video_num = len(whisper_chunks)
        res_frame_queue = queue.Queue()
        self.idx = 0
        
        def process_frames():
            while True:
                if self.idx >= video_num - 1:
                    break
                try:
                    res_frame = res_frame_queue.get(block=True, timeout=1)
                except queue.Empty:
                    continue

                bbox = self.coord_list_cycle[self.idx % len(self.coord_list_cycle)]
                ori_frame = copy.deepcopy(self.frame_list_cycle[self.idx % len(self.frame_list_cycle)])
                x1, y1, x2, y2 = bbox
                
                try:
                    res_frame = cv2.resize(res_frame.astype(np.uint8), (x2 - x1, y2 - y1))
                except:
                    continue
                    
                mask = self.mask_list_cycle[self.idx % len(self.mask_list_cycle)]
                mask_crop_box = self.mask_coords_list_cycle[self.idx % len(self.mask_coords_list_cycle)]
                combine_frame = get_image_blending(ori_frame, res_frame, bbox, mask, mask_crop_box)

                if not skip_save_images:
                    cv2.imwrite(f"{self.avatar_path}/tmp/{str(self.idx).zfill(8)}.png", combine_frame)
                self.idx += 1

        # í”„ë ˆì„ ì²˜ë¦¬ ìŠ¤ë ˆë“œ ì‹œì‘
        process_thread = threading.Thread(target=process_frames)
        process_thread.start()

        # ë°°ì¹˜ ì¶”ë¡ 
        inference_start = time.time()
        gen = datagen(whisper_chunks, self.input_latent_list_cycle, self.batch_size)
        
        for i, (whisper_batch, latent_batch) in enumerate(
            tqdm(gen, total=int(np.ceil(float(video_num) / self.batch_size)), desc="Inference")):
            
            audio_feature_batch = pe(whisper_batch.to(device))
            latent_batch = latent_batch.to(device=device, dtype=unet.model.dtype)

            pred_latents = unet.model(latent_batch, timesteps, 
                                    encoder_hidden_states=audio_feature_batch).sample
            pred_latents = pred_latents.to(device=device, dtype=vae.vae.dtype)
            recon = vae.decode_latents(pred_latents)
            
            for res_frame in recon:
                res_frame_queue.put(res_frame)

        process_thread.join()
        inference_time = time.time() - inference_start
        
        # ë¹„ë””ì˜¤ ìƒì„±
        if out_vid_name and not skip_save_images:
            video_start = time.time()
            cmd_img2video = f"ffmpeg -y -v warning -r {fps} -f image2 -i {self.avatar_path}/tmp/%08d.png -vcodec libx264 -vf format=yuv420p -crf 18 {self.avatar_path}/temp.mp4"
            os.system(cmd_img2video)

            output_vid = os.path.join(self.video_out_path, out_vid_name + ".mp4")
            cmd_combine_audio = f"ffmpeg -y -v warning -i {audio_path} -i {self.avatar_path}/temp.mp4 {output_vid}"
            os.system(cmd_combine_audio)

            os.remove(f"{self.avatar_path}/temp.mp4")
            import shutil
            shutil.rmtree(f"{self.avatar_path}/tmp")
            video_time = time.time() - video_start
            print(f"ğŸ¬ Video generation: {video_time:.2f}s")
        
        total_time = time.time() - total_start
        print(f"ğŸ“Š Performance summary:")
        print(f"   - Audio processing: {audio_time*1000:.1f}ms")
        print(f"   - Neural inference: {inference_time:.2f}s") 
        print(f"   - Total time: {total_time:.2f}s")
        print(f"   - FPS: {video_num/total_time:.1f}")

# ---- 6) FastAPI ì„œë²„ ----
app = FastAPI(
    title="MuseTalk Super Optimized API",
    description="ê·¹ë„ë¡œ ìµœì í™”ëœ ì‹¤ì‹œê°„ MuseTalk inference ì„œë²„",
    version="2.0.0"
)

@app.get("/")
async def root():
    return {
        "message": "MuseTalk Super Optimized API Server",
        "optimizations": [
            "Fast audio processing",
            "Model compilation",
            "Memory optimization", 
            "Batch processing",
            "Precomputed data loading"
        ]
    }

@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "version": args.version,
        "model_ready": True,
        "mode": "super_optimized"
    }

@app.post("/realtime-infer")
async def realtime_infer(
    avatar_id: str = Form(...),
    audio_file: UploadFile = None,
    batch_size: int = Form(20),
    fps: int = Form(25),
    skip_save_images: bool = Form(False),
):
    """ìµœì í™”ëœ ì‹¤ì‹œê°„ inference API"""
    try:
        # ì˜¤ë””ì˜¤ íŒŒì¼ ì„ì‹œ ì €ì¥
        tmp_dir = tempfile.mkdtemp()
        audio_path = os.path.join(tmp_dir, audio_file.filename)
        
        with open(audio_path, "wb") as f:
            f.write(await audio_file.read())

        print(f"ğŸ­ Processing with avatar_id: {avatar_id}")

        # ìµœì í™”ëœ Avatar ì‚¬ìš©
        avatar = SuperOptimizedAvatar(avatar_id=avatar_id, batch_size=batch_size)

        # ìµœì í™”ëœ inference ì‹¤í–‰
        avatar.inference(
            audio_path=audio_path,
            out_vid_name="super_optimized_result",
            fps=fps,
            skip_save_images=skip_save_images,
        )

        # ê²°ê³¼ ë°˜í™˜
        if not skip_save_images:
            output_file = os.path.join(avatar.video_out_path, "super_optimized_result.mp4")
            if not os.path.exists(output_file):
                raise FileNotFoundError(f"Output file not found: {output_file}")
            
            # ì •ë¦¬
            os.remove(audio_path)
            os.rmdir(tmp_dir)
            
            return FileResponse(
                output_file, 
                media_type="video/mp4", 
                filename=f"{avatar_id}_super_optimized.mp4"
            )
        else:
            os.remove(audio_path)
            os.rmdir(tmp_dir)
            return {"message": "Super optimized inference completed", "avatar_id": avatar_id}
        
    except Exception as e:
        print(f"âŒ Error: {str(e)}")
        if 'audio_path' in locals() and os.path.exists(audio_path):
            os.remove(audio_path)
        if 'tmp_dir' in locals() and os.path.exists(tmp_dir):
            os.rmdir(tmp_dir)
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    print("ğŸš€ Starting MuseTalk Super Optimized API Server")
    print("âš¡ Optimizations enabled:")
    print("   - Fast audio feature extraction")
    print("   - Compiled Whisper encoder")
    print("   - Optimized data loading")
    print("   - GPU memory efficiency")
    print("")
    uvicorn.run(app, host="0.0.0.0", port=8000)
